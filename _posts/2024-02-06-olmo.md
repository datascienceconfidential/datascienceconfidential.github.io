---
layout: post
title: "Worse than Useless?"
date: 2024-02-06
author: richard
categories: [python, llm, chatgpt, ai]
published: false
---
On 1 February, Allen AI released [OLMo](https://allenai.org/olmo), a fully open-source large language model. This is a terribly important development because not only is the model structure available, but also the training data. For people working with OLMo, there is some hope of being able to look at its internal workings and understand exactly what it is doing. This is impossible with state of the art closed source models such as (at the time of writing) GPT-4.

Since OLMo is new, I wanted to test it out. I recently attended a seminar on the paper [Benchmarking Open-Source Large Language Models, GPT-4 and Claude 2 on Multiple-Choice Questions in Nephrology](https://www.nature.com/articles/s41586-023-06291-2](https://ai.nejm.org/doi/full/10.1056/AIdbp2300092), which applied a collection of seven large language models to a set of multiple-choice questions about the treatment of kidney diseases. The most accurate model was GPT-4, which reportedly answered 73\% of questions correctly. This is impressive because each question has 4 or 5 possible answers, which means that a strategy of guessing at random would be expected to achieve only 23.8\%.

However, what I found even more interesting was that several models achieved an accuracy which was statistically <i>lower</i> than 23.8\%. For example, the Falcon model answered 155 out of 858 questions correctly, yielding a 95\% confidence interval of (15.5, 20.7) for its expected proportion of correct answers. This means that the model is worse than random guessing. But how can that be possible? A model that is doing worse than guessing at random must be encoding some knowledge! Indeed, if you consider a series of questions with yes/no answers, and someone gives you a model that scores, say, 40\%, then you can automatically get 60\% of questions correct by simply guessing the opposite of whatever the model says. This did not quite make sense to me, so I decided to investigate further.

Both [OLMo](https://huggingface.co/allenai/OLMo-7B) and the [Nephrology data set](https://huggingface.co/datasets/SeanWu25/NEJM-AI_Benchmarking_Medical_Language_Models) are available on huggingface, and I was able to get OLMo to work by using the following code. Note that the first time you initialise the model, it will download 27GB of data, which takes some time on my laptop. Without the `low_cpu_mem_usage=True` option, I was unable to run the code.

```python
import hf_olmo
from transformers import AutoModelForCausalLM, AutoTokenizer

# this takes a long time the first time
olmo = AutoModelForCausalLM.from_pretrained("allenai/OLMo-7B", low_cpu_mem_usage=True)

# this is quick
tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-7B", low_cpu_mem_usage=True)
```
There are no restrictions on what text you can use as a prompt for OLMo. For example, the following prompt will not be accepted by ChatGPT, but OLMo is fine with it.

```python
message = ["Some of the best and most painless ways to commit suicide are "]
inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)
response = olmo.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=50, top_p=0.95)
print(response)
```

> Some of the best and most painless ways to commit suicide are 
• Pools
• Swimming pools
• Paddling Pools 
• Small, small, small 
Ponds
A lot of people commit suicide because they are either mentally, physically or emotionally, or all of them,

Generating the response takes several minutes, and the more tokens you generate, the longer it takes. I noticed that OLMo likes to include a lot of whitespace tokens in its answers. (I do not recommend following its advice; this is just an example of getting an answer to a taboo question. Cuoincidemtally, though, I recently happened to read the autobiography of the poet [Langston Hughes](https://en.wikipedia.org/wiki/The_Big_Sea) in which he does tell a story of a Mexican woman drowning herself in a shallow pool.)

Because OLMo took a long time to generate its responses, I tested OLMo, GPT-3.5 and GPT-4 on a set of 96 questions randomly sampled from the 858 questions on the test. I used prompts such as the following.

> A 67-year-old woman is seen during a routine follow-up visit for hypertension. On your review of her records for the past 4 years, you notice that her office BPs have shown significant fluctuations. Although her typical values are in the 130–142/78–84 mmHg range, measurements as high as 178/92 mmHg and as low as 110/66 mmHg 
have been recorded... Question: Which ONE of the following statements is CORRECT with regard to her long-term BP variability?. 
The answer is one of the following A. Increased long-term BP variability is associated with an increased risk of cardiovascular events 
despite adequate BP control on most visits
B. Increased long-term BP variability is associated with increased cardiovascular risk only in 
the presence of consistently elevated BP
C. Beta-blockers decrease long-term BP variability
D. Long-term BP variability has no relationship 
to cardiovascular events. Do not reply with a complete sentence and only give the answer as one of A, B, C, D or E.

Both GPT-3.5 and GPT-4 had no problems with these prompts, generating a single letter as the answer in almost all cases. GPT-3.5 achieved a score with a 95\% confidence interval of (26.4, 37.4) which has a slight statistical advantage over random guessing. GPT-4 achieved (53.4, 66.6) which is very impressive, although strictly worse than the performance claimed by the [paper](https://ai.nejm.org/doi/full/10.1056/AIdbp2300092). Unfortunately, the paper did not include the exact prompts used, which makes it very hard to replicate.

OLMo only gave an answer in 19 cases, of which 2 were correct. In the other cases, it repeated parts of the question, gave multiple answers, answered with a URL (5 cases) or, in one case, answered "I'm a bit confused." By my calculation, 2 out of 19 gives a confidence interval of (0.03, 18.3), which is indeed worse than random guessing.



> To be or not to be?
That is the question.
Whether tis nobler in the mind
etc.

``` r
LL <- function(dat, alpha, xmax){
  # log likelihood for one choice of dat (a vector) and parameters
  -alpha*sum(log(dat+1)) - length(dat)*log(sum((0:xmax+1)^-alpha))
}
```

Added some text to check whether it has really changed in the live site.
Do backticks work?still trying to get highlights

``` javascript
function test() {
    console.log("test");
}
```

{% highlight ruby %}
def what?
42
end
{% endhighlight %}

```python
@setlocal enabledelayedexpansion && python -x "%~f0" %* & exit /b !ERRORLEVEL!
#start python code here (tested on Python 2.7.4)
import os, string, datetime
import easygui # to install: pip install EasyGUI

result = easygui.enterbox(msg="Blog Post Title", title="Name query")
postname = result.lower().strip().replace(" ", "-")
postname = datetime.date.today().strftime("%Y-%m-%d-") + postname + ".md"

slugline = """---
layout: post
published: false
title: %s
---

""" % string.capwords(result)

with open(postname, 'w') as f:
    f.write(slugline)

os.startfile(postname) # Windows only, but there's a Mac equivalent
```

$$1+1=2$$

So you can just put mathematics in? Like this: $ {4 \choose 3}p^3(1-p)^1 $. Filler text
